{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 3.0 ROS Integration of Object Detection (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this third and final section, you will train an object detection model with NVIDIA DIGITS.  Similar to the image recognition section, you will integrate a pre-trained deployed model into ROS using the `detectnet` ROS node from the `ros_deep_learning` package. You'll test your code in simulation and then deploy it to the physical robot. \n",
    "\n",
    "[3.1 What is Object Detection?](0300_DLforRoboticsObjectDetection#object_detection)<br>\n",
    "[3.2 Object Detection Data](0300_DLforRoboticsObjectDetection#data)<br>\n",
    "[3.3 Object Detection Models](0300_DLforRoboticsObjectDetection#models)<br>\n",
    "[3.4 Inference on the Robot's Jetson](0300_DLforRoboticsObjectDetection#inference)<br>\n",
    "[3.5 Integrating Inference into ROS](#integration)<br>\n",
    "[3.6 Your ROS Object Detection node: bot-nav detector](#simulation)<br>\n",
    "[3.7 Assessment](#assessment)<br>\n",
    "[3.8 Deployment](#deployment)<br>\n",
    "[3.9 Recap](#recap)<br>\n",
    "[Appendix](9900_DL4R_Appendix.ipynb#apx-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='integration'></a>\n",
    "# 3.5 Integrating Object Detection into ROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `detectNet` vision primitive\n",
    "Object Detection inference on the Jetson is built around the the Deep Vision [detectNet object](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/classdetectNet.html).  As with classification, the supporting software is installed from the `jetson-inference` library on the robot's Jetson. Instead of a number and confidence level, the neural network for `detectnet` node outputs an array of detections with bounding boxes, one for each instance of the an object found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The detection message\n",
    "The `image-publisher` node publishes the images from the camera on the `/image_raw` topic.  The `imagenet` node in the `ros_deep_learning` package initializes and reads the the neural network inference, and then publishes the results on the `/detectnet/detections` topic in the [vision_msgs/Detection2DArray](http://docs.ros.org/api/vision_msgs/html/msg/Detection2DArray.html) format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/visionmsg_detect.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our original goal is to have the robot find an object and move toward it.  The detection message is an array of **detections**.  For each detection, we are interested in the **results[]** array and the **bbox** bounding box.  This tells us what the object is and where it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='simulation'></a>\n",
    "# 3.6 Your ROS Object Detection node: `bot-nav detector`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nodes-detector.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open your `detector` Python file, run the following cell to create a symbolic link from this notebook environment to the ROS workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -sv ~/AionR1_ws/src/bot-nav/scripts/section3/detector detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, click the `File/Open` drop-down command from the navigation on this notebook page.  You'll see a list of all the notebooks for the course as well as the symbolic link you just created.  Click on the `detector` link to open the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportional controller\n",
    "We are using a proportional controller (P-controller) to drive towards an object. Linear scaling of an error drives the control output. In our case, the error signal is the distance between the centerline of the image and the center of the object we are trying to approach.  The code for this is found in the `drive_to_target` definition in the `detector` python file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing your node with ROS bags\n",
    "As with classification, you'll check your code with ROS bags.  This time though, you'll be able to use the Gazebo simulation once again for better visualization.  The ROS bags are just pre-determined recordings of a successful run with a robot that moved toward the objects as expected.  If your code works correctly, you'll see the robot in simulation converge toward the object.  If the code is not right, there will be a mismatch and this should be obvious in the visual representation.  If your robot doesn't move toward the object marker, then something is wrong!\n",
    "\n",
    "You may want to look back at the [instructions from the previous section](0230_InferenceOnJetson.ipynb#simulation) for hints on what nodes to run and topics to echo in order to test your `detector` node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/detector_moving_in_gazebo.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assessment'></a>\n",
    "# 3.7 Assessment\n",
    "### Section 3 Coding: Identify the left bottle position so the robot can move toward it.\n",
    "\n",
    "Follow the \"TODO\" instructions in the `detector` Python ROS node file. Test your code with the ROS bags, either directly or using the bot-nav interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "...\n",
    "    # Get the object from the object_position\n",
    "    # object_position can be 'first' 'left' or 'right'\n",
    "    # Note that this is a naive implementation\n",
    "    def get_object_position(self, object_position):\n",
    "        if self.detection_result is None:\n",
    "            return None\n",
    "        if (len(self.detection_result.detections) == 1):\n",
    "            return self.detection_result.detections[0]\n",
    "        if object_position == 'first':\n",
    "            return self.detection_result.detections[0]\n",
    "        if object_position == 'left':\n",
    "            # TODO complete the code find the left-most object\n",
    "            # ********your code starts here - do not modify code above here\n",
    "\n",
    "            return None  # FIXME\n",
    "            # ********your code ends here - do not modify code below here\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your code is working correctly on the desktop, you can submit it for course assessment.  \n",
    "First, copy your `detector` node to the assessment folder by executing the following copy command. You'll need to do this any time you change your code and want to resumbit for grading.   You can do this as many times as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -v ~/AionR1_ws/src/bot-nav/scripts/section3/detector assessment_export_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, return to the classroom launch page, and click the the checkmark titled \"ASSESS TASK\" to run the grading assessment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/assess_task.png\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deployment'></a>\n",
    "# 3.8 Deployment\n",
    "Deployment for this section works the same as with section 1.  You'll deploy in small groups - just follow the classroom instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='recap'></a>\n",
    "# 3.9 Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations!  You have successfully written a classification node to detect objects inferred from a DNN model, and responded to that perception with action!  That is truly autonomous movement in the robot. Specifically, you've learned to:\n",
    "* Train an Object Detection (localization) model with DIGITS\n",
    "* Access the detectnet inference stream through ROS\n",
    "* Analyze the inference stream to determine where a specific object is located in a scene\n",
    "* Deploy the DNN models and the ros_deep_learning package to the robot\n",
    "* Integrated deep learning with ROS!\n",
    "\n",
    "With the tools you've learned in this course, you can begin to build your own projects using object detection and ROS. Check out the Appendix for more links and information on the topics in the course.  Good luck with your next robot porject!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid black; background-color:#f2f2f2; padding:10px; width:400px; margin:auto;\">\n",
    "<b><a href=\"DLI Welcome Notebook.ipynb\">Overview</a></b><br>\n",
    "<b><a href=\"0100_DLforRoboticsControl.ipynb\">1.0 Introduction to ROS Control</a></b><br>\n",
    "<b><a href=\"0200_DLforRoboticsImageRecognition.ipynb\">2.0 ROS Integration of Image Recognition</a></b><br>\n",
    "<b><a href=\"0300_DLforRoboticsObjectDetection.ipynb\">3.0 ROS Integration of Object Detection</a></b><br>\n",
    "<b><a href=\"9900_DL4R_Appendix.ipynb\">Appendix</a></b><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
