{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 3.0 ROS Integration of Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this third and final section, you will train an object detection model with NVIDIA DIGITS.  Similar to the image recognition section, you will integrate a pre-trained deployed model into ROS using the `detectnet` ROS node from the `ros_deep_learning` package. You'll test your code in simulation and then deploy it to the physical robot. \n",
    "\n",
    "[3.1 What is Object Detection?](#object_detection)<br>\n",
    "[3.2 Object Detection Data](#data)<br>\n",
    "[3.3 Object Detection Models](#models)<br>\n",
    "[3.4 Inference on the Robot's Jetson](#inference)<br>\n",
    "[3.5 Integrating Inference into ROS](0350_IntegratingObjectDetectionROS.ipynb#integration)<br>\n",
    "[3.6 Your ROS Object Detection node: bot-nav detector](0350_IntegratingObjectDetectionROS.ipynb#simulation)<br>\n",
    "[3.7 Assessment](0350_IntegratingObjectDetectionROS.ipynb#assessment)<br>\n",
    "[3.8 Deployment](0350_IntegratingObjectDetectionROS.ipynb#deployment)<br>\n",
    "[3.9 Recap](0350_IntegratingObjectDetectionROS.ipynb#recap)<br>\n",
    "[Appendix](9900_DL4R_Appendix.ipynb#apx-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='object_detection'></a>\n",
    "# 3.1 What is Object Detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Object Detection** classifies objects, but goes a step further in that it localizes them within the image.  This way, multiple objects can be detected simultaneously.  Object detection is one of the most challenging problems in computer vision and is the first step in several computer vision applications.<br>\n",
    "The goal of an object detection system is to detect all instances of objects of a known category in an image.\n",
    "\n",
    "<img src=\"images/trucks_bb.png\" alt=\"object_detection\" width=600/>\n",
    "The training workflow for object detection with DIGITS is the same as the training for an image classification network model.  \n",
    "\n",
    "1. Create a dataset for a certain object detection task\n",
    "- Define an object detection network model\n",
    "- Train the model with the dataset\n",
    "\n",
    "In this lab, we won't be able to actually run the training of the object detection model due to time constraints.<br>\n",
    "However, we have some pre-created datasets and models, so we can see look at some examples of object detection training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "# 3.2 Object Detection Data\n",
    "Object Detection data includes more than just object labels.  It includes localization information, generally in the form of bounding boxes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Datasets for Object Detection\n",
    "\n",
    "There are some publicly available datasets that contain data for object detection.\n",
    "\n",
    "- [MS COCO](http://cocodataset.org/#home)\n",
    "- [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/)\n",
    "- [KITTI](http://www.cvlibs.net/datasets/kitti/)\n",
    "- [Open Images](https://github.com/openimages/dataset)\n",
    "\n",
    "The datasets we will look at are derived from the COCO (Common Objects in Context) data. By going to the website, you can browse the various datasets and see visualizations.  \n",
    "\n",
    "<img src=\"images/coco1.png\" alt=\"coco\" width=600/>\n",
    "<img src=\"images/coco2.png\" alt=\"coco\" width=600/>\n",
    "\n",
    "Although the browsing tool for COCO provides a robust visualization with many vertices, the Object Detection network we will use in DIGITS uses bounding box information. This information is also provided in the COCO data, but needs to be reformatted to the KITTI format for use with DIGITS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KITTI data format\n",
    "\n",
    "For more detail of the KITTI data format, please check the README on Object Detection in the NVIDIA DIGITS repo [here](https://github.com/NVIDIA/DIGITS/blob/digits-6.0/digits/extensions/data/objectDetection/README.md).\n",
    "\n",
    "The key information in the KITTI text file is 4 `bbox` fields which contain the information for a 2D bounding box of the object in the image: values are the left, top, right, and bottom pixel coordinates.  Note that the (0,0) location of an image is the upper left corner.\n",
    "\n",
    "<img src=\"images/kittiformat.png\" alt=\"kitti\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion\n",
    "Information on downloading and converting additional COCO data subsets into KITTI format can be found [here](https://github.com/dusty-nv/jetson-inference#downloading-the-detection-dataset) in the `jetson-inference` repo. \n",
    "A script, [coco2kitti.py](https://github.com/dusty-nv/jetson-inference/blob/master/tools/coco2kitti.py), is also available in the repo.\n",
    "\n",
    "Explore converted COCO datasets provided below, ready for DIGITS: airplane, bottle, chair, and dog.  Execute the following cells to view the locations and examples.  Feel free to change the cells so you can look at different data subsets, images, and corresponding bounding box label text files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -d /dli/data/coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /dli/data/coco/train/images/airplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"/dli/data/coco/train/images/airplane/110203.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /dli/data/coco/train/labels/airplane/110203.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Dataset\n",
    "\n",
    "### <a href=\"/digits/\" target=\"_blank\">OPEN DIGITS</a>\n",
    "Click the link above to launch DIGITS if necessary.<br>\n",
    "Keep both browser windows open (this notebook and DIGITS) so that you can refer to these instructions as you use the DIGITS tool. \n",
    "\n",
    "<img src=\"images/digits_cocosets.png\" alt=\"digits_datasets\" width=800/>\n",
    "\n",
    "Click `Clone Job` to see how a KITTI dataset is constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/od_construction.png\" alt=\"od_construction\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new dataset using the `bottle` data, complete the form using the following values and click \"create\":<br>\n",
    "\n",
    "|Field|Path|\n",
    "|-----|-----|\n",
    "|Feature image folder |`/dli/data/coco/train/images/bottle`|\n",
    "|Label image folder |`/dli/data/coco/train/labels/bottle`|\n",
    "|Validation feature image folder |`/dli/data/coco/val/images/bottle`|\n",
    "|Validation label image folder |`/dli/data/coco/val/labels/bottle`|\n",
    "|Custom classses|`dontcare, bottle`|\n",
    "|Dataset Name|`COCO-bottle`|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='models'></a>\n",
    "# 3.3 Object Detection models\n",
    "\n",
    "Open DIGITS, and click on the \"Model\" tab.<br>\n",
    "There are 7 jobs, which are all object detection model training runs done on subsets of the COCO dataset.\n",
    "\n",
    "When you build jobs yourself, be sure to give unique and identifiable names to each job, showing some parameters and results of the training.\n",
    "\n",
    "<img src=\"images/digits_od_models.png\" alt=\"od_construction\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the first model in the list.<br>\n",
    "On the Loss-Accuracy graph, select the Epoch #3 to #100 to have better visibility of the curve. The very beginning includes a spike that overshadows later values.  To do this, move your cursor over the 0-100 mini-graph directly under the Loss-Accuracy graph.  The grayed area will appear showing the range for the main graph.\n",
    "<img src=\"images/select_3to100.png\" width=600/>\n",
    "<img src=\"images/od_detectnet.png\" alt=\"od_detectnet\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how this training job was configured, click on \"Clone\" job.\n",
    "\n",
    "You may notice that there is a list of 3-4 training jobs for the same combination of network and the data class.\n",
    "As suggested by the last part of the name, they are trained with slightly different parameters.\n",
    "\n",
    "Check how those changes affected the model training result. In practice, experimentation is necessary to find a good combination of parameters for your dataset and model.\n",
    "<img src=\"images/iteration.png\" alt=\"iteration\" width=400/>\n",
    "\n",
    "To create your own models, refer to the instructions titled [Creating DetectNet Model with DIGITS](https://github.com/dusty-nv/jetson-inference/blob/master/docs/detectnet-training.md#creating-detectnet-model-with-digits) found in the `jetson-inference` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='inference'></a>\n",
    "# 3.4 Inference on the Robot's Jetson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with image recognition models, you can use DIGITS features to test the trained model with static images or a camera stream. Once you build a model with acceptable accuracy, you can deploy it to the Jetson.  For this project, a model that detects bottles trained from the COCO bottle subset ***has already been deployed to the Jetson on the R1 robot.*** \n",
    "\n",
    "For information on how to deploy the model to the Jetson, please see the [Appendix](9900_DL4R_Appendix.ipynb#apx-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='integration'></a>\n",
    "## [--> Next: 3.5 Integrating Object Detection into ROS](0350_IntegratingObjectDetectionROS.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid black; background-color:#f2f2f2; padding:10px; width:400px; margin:auto;\">\n",
    "<b><a href=\"DLI Welcome Notebook.ipynb\">Overview</a></b><br>\n",
    "<b><a href=\"0100_DLforRoboticsControl.ipynb\">1.0 Introduction to ROS Control</a></b><br>\n",
    "<b><a href=\"0200_DLforRoboticsImageRecognition.ipynb\">2.0 ROS Integration of Image Recognition</a></b><br>\n",
    "<b><a href=\"0300_DLforRoboticsObjectDetection.ipynb\">3.0 ROS Integration of Object Detection</a></b><br>\n",
    "<b><a href=\"9900_DL4R_Appendix.ipynb\">Appendix</a></b><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
