{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 2.0 ROS Integration of Image Recognition (continued)\n",
    "\n",
    "[2.1 What is Image Recognition?](0200_DLforRoboticsImageRecognition.ipynb#image_recognition)<br>\n",
    "[2.2 Image Recognition Models](0200_DLforRoboticsImageRecognition.ipynb#models)<br>\n",
    "[2.3 Inference on the Robot's Jetson](#inference)<br>\n",
    "[2.4 Integrating Inference into ROS](#integration)<br>\n",
    "[2.5 Your ROS Image Recognition node: bot-nav classifier](#simulation)<br>\n",
    "[2.6 Assessment](#assessment)<br>\n",
    "[2.7 Deployment](#deployment)<br>\n",
    "[2.8 Recap](#recap)<br>\n",
    "[Appendix](9900_DL4R_Appendix.ipynb#apx-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations!  You've successfully trained a classification deep learning model for image recognition!   This is the process you can follow if you have a dataset you want to train for your project.  For our robot project, we are have a trained model already loaded.  We'll use a GoogleNet model that is pre-trained on [1000 classes of objects](https://github.com/dusty-nv/jetson-inference/blob/master/data/networks/ilsvrc12_synset_words.txt) from the ImageNet ILSVRC12 benchmark. The model is more complex and takes longer to train, but the principle is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='inference'></a>\n",
    "# 2.3 Inference on the Robot's Jetson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run inference on our robot, we need:\n",
    "\n",
    "1. Inference software to run the neural network models on the NVIDIA Jetson TX2 GPU\n",
    "2. A trained model downloaded from DIGITS\n",
    "3. Software to translate the inference results into a ROS topic we can work with\n",
    "\n",
    "The inference software and pre-trained models have already been deployed to the robot's Jetson (steps 1 and 2). The inference software is available from the the [jetson-inference](https://github.com/dusty-nv/jetson-inference#downloading-model-snapshot-to-jetson) repository, which includes detailed instructions and guides for running inference on a Jetson. Basic instructions are also provided in the [Appendix](9900_DL4R_Appendix.ipynb#apx-2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the `jetson-inference` library\n",
    "The [jetson-inference](https://github.com/dusty-nv/jetson-inference#downloading-model-snapshot-to-jetson) repository uses [NVIDIA TensorRT](https://developer.nvidia.com/tensorrt) for efficiently deploying neural networks onto the embedded platform, improving performance and power efficiency using graph optimizations, kernel fusion, and half-precision FP16 on the Jetson.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/images/deep-vision-primitives.png)\n",
    "\n",
    "Vision primitives, such as `imageNet` for image recognition, `detectNet` for object localization, and `segNet` for semantic segmentation, inherit from the shared `tensorNet` object. Examples are provided for streaming from live camera feed and processing images from disk. See the [Deep Vision API Reference Specification](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/index.html) for accompanying documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='integration'></a>\n",
    "# 2.4 Integrating Inference into ROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `imageNet` vision primitive\n",
    "Image recognition inference on the Jetson is built around the the Deep Vision [imageNet object](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/classimageNet.html).  The supporting software is installed from the `jetson-inference` library on the robot's Jetson. This makes it possible to create a neural network on the Jetson with a few programmatic commands in C++ and start the classifier on an image, or series of images in the case of a video stream.  The neural network outputs a number corresponding to which image identification is most likely, along with a confidence level.\n",
    "\n",
    "Integration with ROS entails \"wrapping\" the calls in a ROS node.  This capability is included in the [ros_deep_learning](https://github.com/dusty-nv/ros_deep_learning) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the `ros_deep_learning` package\n",
    "\n",
    "This repo contains deep learning inference nodes for ROS with support for NVIDIA Jetson TX1/TX2/Xavier and TensorRT.\n",
    "\n",
    "The nodes use the image recognition and object detection vision objects from the `jetson-inference` library and [NVIDIA Two Days to a Demo](https://developer.nvidia.com/embedded/twodaystoademo) tutorial, which come with several built-in pretrained network models and the ability to load customized user-trained models. ROS Kinetic (for TX1/TX2) and ROS Melodic (for Xavier) are supported.\n",
    "\n",
    "Additional ROS package dependencies installed on the robot are:\n",
    "* [image-transport](http://wiki.ros.org/image_transport)\n",
    "* [image-publisher](http://wiki.ros.org/image_publisher)\n",
    "* [vision-msgs](http://wiki.ros.org/vision_msgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The classification message\n",
    "The `image-publisher` node publishes the images from the camera on the `/image_raw` topic.  The `imagenet` node in the `ros_deep_learning` package initializes and reads the the neural network inference, and then publishes the results on the `/imagenet/classification` topic in the [vision_msgs/Classification2D](http://docs.ros.org/melodic/api/vision_msgs/html/msg/Classification2D.html) format.  To see what this format looks like, use the [rosmsg show](http://wiki.ros.org/rosmsg) command in a desktop terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/visionmsg.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the message is \"called back\" in the node's subscriber, the elements we are interested in can be referenced.  For example, suppose your classification node is subscribing to this topic and you want to know what the result was. That information is in the ObjectHypothesis array. You could extract that information like so:\n",
    "\n",
    "```python\n",
    "my_result = msg.results[0]\n",
    "rospy.loginfo(\"Classify callback ID: %d score: %f\", my_result.id, my_result.score)\n",
    "```\n",
    "\n",
    "The rest of the message isn't needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='simulation'></a>\n",
    "# 2.5 Your ROS Image Recognition node: `bot-nav classifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nodes-classifier.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bot-nav classifier node subscribes to the `/imagenet/classification` topic.  The idea is that it could also publish a movement of some sort based on what image it sees.  For example if it sees a bear, it might quickly turn around and drive away!  This could be done by publishing the appropriate movement to the `/cmd_vel` topic, just as you did in section 1.  For now though, we just want to be sure that the node is properly analyzing the classification message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open your `classifier` Python file, run the following cell to create a symbolic link from this notebook environment to the ROS workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -sv ~/AionR1_ws/src/bot-nav/scripts/section2/classifier classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, click the `File/Open` drop-down command from the navigation on this notebook page.  You'll see a list of all the notebooks for the course as well as the symbolic link you just created.  Click on the `classifier` link to open the file editor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing your node with ROS bags\n",
    "Our Gazebo simulation does not have a photorealistic view that can be used for actual inference in our simulation.  We can still test our responses to the `/imagenet/classification` topic input stream with the help of [ROS bags](http://wiki.ros.org/Bags).  With this tool, we can create a recording of everything a robot senses, save it, and then play the recording back.  Some ROS bags have been recorded for you for a few different images, and these can be \"played\" in the ROS network so that your node can respond to the messages in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ~/bags/classify*.bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these bags contains a recording of the `/imagenet/classification` topic while the robot is viewing an object.  For example `classify1.bag` records the inference while continuously viewing a water bottle, which is class #898 in the [list of 1000 classified objects](https://github.com/dusty-nv/jetson-inference/blob/master/data/networks/ilsvrc12_synset_words.txt) (see line #899 of the list).   The only `id` the classifier should get is this water bottle! More information on the bag recordings can be found in the `~/Classifier Info` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/classify2-898-water-bottle.jpg\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the ROS bag in action, you will need to start up several nodes and topic echo processes, each with its own terminal.  The first one is the ROS master node with the `roscore` command.  We didn't need this before because it was automatically started with the Gazebo launch, but we need it now since we are running ROS without Gazebo.  Here are the processes you need:\n",
    "\n",
    "* new terminal\n",
    "\n",
    "<code  style=\"\n",
    "  border:2px solid gray;\n",
    "  background-color:black;\n",
    "  color:white;\n",
    "  display: block;\n",
    "  padding:10px; \n",
    "  width:200px; \n",
    "  \">roscore\n",
    "</code>\n",
    "    \n",
    "* new terminal\n",
    "\n",
    "<code  style=\"\n",
    "  border:2px solid gray;\n",
    "  background-color:black;\n",
    "  color:white;\n",
    "  display: block;\n",
    "  padding:10px; \n",
    "  width:600px; \n",
    "  \">rosrun bot-nav classifier /cmd_vel:=/cmd_vel_mux/input/teleop\n",
    "</code>\n",
    "    \n",
    "* new terminal\n",
    "\n",
    "<code  style=\"\n",
    "  border:2px solid gray;\n",
    "  background-color:black;\n",
    "  color:white;\n",
    "  display: block;\n",
    "  padding:10px; \n",
    "  width:400px; \n",
    "  \">rostopic echo /imagenet/classification\n",
    "</code>\n",
    "    \n",
    "* new terminal\n",
    "\n",
    "<code  style=\"\n",
    "  border:2px solid gray;\n",
    "  background-color:black;\n",
    "  color:white;\n",
    "  display: block;\n",
    "  padding:10px; \n",
    "  width:400px; \n",
    "  \">rosbag play ~/bags/classify2.bag\n",
    "</code>\n",
    "    \n",
    "You still need one more thing.  Your `classifier` node is also subscribing to the `/remote_commad` topic.  It won't start classifying until it knows it is in \"guided\" mode and has received a \"classify\" command,  To send these commands, use the [rostopic pub](http://wiki.ros.org/rostopic#rostopic_pub) command in yet another new terminal as follows:\n",
    "\n",
    "* new terminal\n",
    "\n",
    "<code  style=\"\n",
    "  border:2px solid gray;\n",
    "  background-color:black;\n",
    "  color:white;\n",
    "  display: block;\n",
    "  padding:10px; \n",
    "  width:600px; \n",
    "  \">rostopic pub /remote_command std_msgs/String \"data: 'guided'\" \n",
    "</code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/classifier_rosbag_test.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bot-nav interface can also be used to launch rosbags.  These are set up under the Section 2 tab:\n",
    "\n",
    "<code  style=\"\n",
    "  border:2px solid gray;\n",
    "  background-color:black;\n",
    "  color:white;\n",
    "  display: block;\n",
    "  padding:10px; \n",
    "  width:200px; \n",
    "  \">bni\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assessment'></a>\n",
    "# 2.6 Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 Coding: Identify what object was found on the inference classification topic\n",
    "\n",
    "Follow the \"TODO\" instructions in the `classifier` Python ROS node file.  Test your code using ROS bags either directly, or using the bot-nav interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "...\n",
    "    @staticmethod\n",
    "    def parse_results(classify_data):\n",
    "        # TODO complete the code to build the loginfo printout of the classifier result\n",
    "        # ********your code starts here - do not modify code above here\n",
    "        # extract the results (ID and score) from the message\n",
    "        # note that you only need to look at the first classification\n",
    "        # print out the results with the loginfo command\n",
    "        # return the results instead of \"None\"\n",
    "\n",
    "        return None  # FIXME\n",
    "        # ********your code ends here - do not modify code below here\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your code is working correctly on the desktop, you can submit it for course assessment.  \n",
    "First, copy your `classifier` node to the assessment folder by executing the following copy command. You'll need to do this any time you change your code and want to resumbit for grading.   You can do this as many times as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -v ~/AionR1_ws/src/bot-nav/scripts/section2/classifier assessment_export_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, return to the classroom launch page, and click the the checkmark titled \"ASSESS TASK\" to run the grading assessment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/assess_task.png\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deployment'></a>\n",
    "# 2.7 Deployment\n",
    "\n",
    "We won't deploy the `classification` node.  Why is that?  The imagenet model can recognize 1000 objects!  Unfortunately, it must classify the whole image within its camera view as an object.  We need to know _where the object is located,_ in order to move toward it.\n",
    "\n",
    "You have all the building blocks now though to do just that!  All we need is a more complex inference model, which we'll cover in Section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='recap'></a>\n",
    "# 2.8 Recap\n",
    "\n",
    "Congratulations!  You have successfully written a classification node to recognize images inferred from a DNN model, and _make that information accessible_ to the robot logic.  Specifically, you've learned to:\n",
    "* Train an Image Recognition (classification) model with DIGITS\n",
    "* Access the imagenet inference stream through ROS\n",
    "* Analyze the inference stream to determine what the robot has inferred from the scene\n",
    "* Use ROS bags to test your code\n",
    "\n",
    "Up next, you'll take this a step further with a more complex inference model and build a node that can recognize an object and move the robot in response! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [--> Next: 3.0 ROS Integration of Object Detection](0300_DLforRoboticsObjectDetection.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid black; background-color:#f2f2f2; padding:10px; width:400px; margin:auto;\">\n",
    "<b><a href=\"DLI Welcome Notebook.ipynb\">Overview</a></b><br>\n",
    "<b><a href=\"0100_DLforRoboticsControl.ipynb\">1.0 Introduction to ROS Control</a></b><br>\n",
    "<b><a href=\"0200_DLforRoboticsImageRecognition.ipynb\">2.0 ROS Integration of Image Recognition</a></b><br>\n",
    "<b><a href=\"0300_DLforRoboticsObjectDetection.ipynb\">3.0 ROS Integration of Object Detection</a></b><br>\n",
    "<b><a href=\"9900_DL4R_Appendix.ipynb\">Appendix</a></b><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
